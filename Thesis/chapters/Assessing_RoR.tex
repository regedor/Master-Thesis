\thispagestyle{empty}
\chapter{Assessing Ruby on Rails Projects}\label{chap:assissing_ror}

To assess the quality of a software project is not an easy task.
To many variables should be taken into consideration and most of their values can be considered subjective.

Our thesis is that an open source project quality is somehow influenced by the best practices followed 
by its contributors.

Furthermore, since Rails community has been our object of study we decided to conduct a series of studies, 
with the objective of finding correlations between best practices defined by the Rails community and
the quality of projects following it.

With the objective of automatically verify whether or not best practices are being followed by
a given Rails project, the open source ruby gem rails\_best\_practices 
was created (by the authors of Rails best practices web site).
We agreed on using it as starting point for our work.

This chapter reports the different studies carried out, the main difficulties and the results obtained.



\section{First Study}\label{subsec:first_study}
One of the first things that we have noticed when we applied this gem to OSS projects,
is that the biggest and most renown projects have much more errors than the smaller and unknown projects.
This nonsense has a simple  interpretation.
Small projects (like the majority of Rails projects found in GitHub) are simple software packages,
often developed by a single user, projects carried out for simple learning purposes.
These applications are so simple that many times the code is almost entirely created by Rails code generators.
Usually, when code is not written by humans, it has few mistakes concerning those recommendations.

Having taken the above into account, we decided to run the rails best practices gem on similar Rails projects.
Seven \emph{time tracking} or \emph{project management} open source systems were chosen.
After running the gem and counting the
\textsf{not best practices (NBPs)}\footnote{In fact, Rails best practices gem does not find best practices in the source code.
  It does the opposite, it discovers when the code is not written according to a best practice, in other words, 
  it identifies bad practices (similar to the detection of code smells).
  We decided to name those occurrences NBP.
}
occurrences, the following results were obtained:

\input{tables/RBPResults1Raw}

Rubytime seems to have the best results and Clockingit the worst. 
The fact is that very good user reviews can be found about Rubytime.
However, Tracks obtained an unexpected high score, since it has been very sparsely maintained 
(old code has higher probability of not following the current best practices).
As explained before, those values are not really measuring if a project follows best practices 
but instead measuring when it fails.
This should also be taken into consideration. 

The most evident problem here is that best practices are not being weighted and neither the size of the project considered.
For instance, if the developers have the habit of leaving trailing white spaces, 
the occurrences of this will obviously be related to the size of the project.
On the other hand, it is a best practice to remove the default route generated by rails, 
independently of the project size this is true or false, there is no way to leave the route two times. 
So, if developers do not take into account those two best practices, when the project grows, 
the number of trailing spaces will increase and the results will show more NBPs, 
but the other one will always be only one NBP.  
Because of that we can get twisted results.

To avoid this, the projects were sized.
The size attribute is based on the quantity of models and controllers in the project.
After that, we divided the values previously obtained  by the project size.
By doing that, a new set of results emerge.

\input{tables/RBPResults1}

Those results are much more likely to be helpful in terms of understanding if a project is or is not following 
best practices.
The numbers reflect both the community reviews and our own estimates much more.


\section{Second Study}\label{subsec:second_study}
After the first study reported above, we felt that it was time to make a bigger one;
we should repeat the experiment over a larger sample. 
In addition, there was the need to define an objective quality metric to compare the metrics results with.
As a second target for this new phase, it was decide to find an objective quality rate (a reputation ranking) for each project in the sample, 
to be possible to compare with the results computed for the best practices metrics.

For the second study, we selected 40 Ruby on Rails projects hosted in github and
decided to consider the number of 
\textsf{followers}\footnote{Number of users that want to receive notifications about the project.} and
\textsf{forks}\footnote{Number of people that forked the project. This means that either they want to contribute to the project or create a derived project}, 
that each project has on github, 
as a \emph{project reputation} metric. 

The objective was to prove that a negative correlation exists, between the NBPs of a project and its followers and forks. 

The previous study has shown us the need to apply different weights to each NBP. 
By diving the NBPs by the project size, in the first study, seemed like we got better results.
However, not all NBPs depend on the project size. 
Therefore, we altered the rails best practices gem to make it possible to know how much project files were analyzed 
by each rails best practice checker.

Basically, after collecting the GitHub URLs for each project, we followed the next steps:
\begin{itemize}
\item \emph{Retrieve GitHub information}, in this step we get the followers and forks(and more info that might be used in further analyses).
\item \emph{Download the project repository}.
\item \emph{Run rails best practices gems}, at this point, we get the non weighted NBPs and files given by each one of the 29 checkers.
\item \emph{Calculate the Weighted Global NBPs}, the evaluation algorithm consists in dividing the value returned by each NBP checker  by the number of files checked and, then sum it.
\end{itemize}

Next, an excerpt of the obtained table is shown:
\input{tables/RBPResults2Raw}

\section{Results}\label{subsec:results}
After building the table containing the results for the 40 projects, we easily found correlations between columns.
We discovered that the average correlation index, for the weighted C(x) columns, is -0.2. Only three of the weighted C(x) columns do not have negative correlation. This is quite good, considering the fact that there is an explanation for it. 
Those three checkers (without negative correlation) aimed at finding  NBPs that almost non of the projects were committing, 
so there is no correlation.


The most important results are in the next table:
\input{tables/RBPResults2}

These correlation indexes show that if we just count the nbps there is no relation between them and the number of forks and watchers. Nevertheless, the Weighted NBPs have a quite perceptible negative correlation both with watchers and forks. 

Observing that Table, it is possible to notice that the forks correlation is bigger. 
We believe that if it happens, it is because forking a project shows intensions of digging into the code and, 
of course, it easier to understand others code when it follows good practices.




\section{Third Study}\label{subsec:thrid_study}
After the previous studied, it turned out that we have a achieved a new metric for classifying the quality of Rails projects.
By inverting the total number of NBPs and scaling it, we can create a tool to automatically give a score to the projects.

That is exactly what we did, we created a simple application that receives the path for a GitHub rails project. 
As result by automating everything, it was possible to run the application against a bigger set of projects.
A list of analyzed projects can be found at \url{www.study.gourgeouscode.com}.










