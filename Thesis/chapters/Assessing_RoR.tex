\thispagestyle{empty}
\chapter{Assessing Ruby on Rails Projects}\label{chap:assissing_ror}

To assess the quality of a software project is not an easy task.
To many variables should be taken into consideration and most of their values can be considered subjective.

Our thesis is that the quality of an open source project is somehow influenced by the best practices followed 
by its contributors.

Furthermore, since Rails community has been our object of study we decided to conduct a series of studies, 
with the objective of finding correlations between best practices defined by the Rails community and
the quality of projects following it.

With the objective of automatically verify whether or not best practices are being followed by
a given Rails project, the open source ruby gem rails\_best\_practices 
was created (by the authors of Rails best practices web site).
We agreed on using it as starting point for our work.

This chapter reports the different studies carried out, the main difficulties and the results obtained.



\section{First Study}\label{subsec:first_study}
One of the first things that we have noticed, when we applied the rails\_best\_practices gem to OSS projects,
is that the biggest and most renown projects have much more errors than the smaller and unknown projects.
This nonsense has a simple  interpretation.
Small projects (like the majority of Rails projects found in GitHub) are simple software packages,
often developed by a single user, projects carried out for simple learning purposes.
These applications are so simple that many times the code is almost entirely created by Rails code generators.
Usually, when code is not written by humans, it has few mistakes concerning those recommendations.

Having taken the above into account, we decided to run the rails best practices gem on similar Rails projects.
Seven \emph{time tracking} or \emph{project management} open source systems were chosen.
After running the gem and counting the
\textsf{not best practices (NBPs)}\footnote{In fact, Rails Best Practices gem does not find best practices in the source code.
  It does the opposite, it discovers when the code is not written according to a best practice, in other words, 
  it identifies bad practices (similar to the detection of code smells).
  We decided to name those occurrences NBP.
}
occurrences, the following results were obtained:

\input{tables/RBPResults1Raw}

Rubytime seems to have the best results and Clockingit the worst. 
The fact is that very good user reviews can be found about Rubytime.
However, Tracks obtained an unexpected high score, since it has been very sparsely maintained 
(old code has higher probability of not following the current best practices).
As explained before, those values are not really measuring if a project follows best practices 
but instead measuring when it fails.
This should also be taken into consideration. 

The most evident problem here is that best practices are not being weighted and neither the size of the project considered.
For instance, if the developers have the habit of leaving trailing white spaces, 
the occurrences of this will obviously be related to the size of the project.
On the other hand, it is a best practice to remove the default route generated by rails, 
independently of the project size this is true or false, there is no way to leave the route two times. 
So, if developers do not take into account those two best practices, when the project grows, 
the number of trailing spaces will increase and the results will show more NBPs, 
but the other one will always be only one NBP.  
Because of that we can get twisted results.

To avoid this, the projects were sized.
The size attribute is based on the quantity of models and controllers in the project.
After that, we divided the values previously obtained  by the project size.
By doing that, a new set of results emerge.

\input{tables/RBPResults1}

Those results (in Table~\ref{tab:rbpresults_1}) are much more likely to be helpful in terms of understanding if a project is or is not following 
best practices.
The numbers reflect both the community reviews and our own estimates much more.



\section{Second Study}\label{subsec:second_study}
After the first study reported above, we felt that it was time to make a bigger one;
we should repeat the experiment over a larger sample. 
As a second target for this new phase, it was decide to find an objective quality rate (a reputation ranking) 
for each project in the sample.
There was the need to define an objective quality metric to compare the metrics results with.
This way it would be possible to prove that there is a statistical relation between the quality of the project
and the results of our best practices metric based on NBPs.

For the second study, we selected 40 Ruby on Rails projects hosted in github and
decided to consider the number of 
\textsf{followers}\footnote{Number of users that want to receive notifications about the project.} and
\textsf{forks}\footnote{Number of people that forked the project. This means that either they want to contribute to the project or create a derived project}, 
that each project has on github, 
as a \emph{project reputation} metric. 

The objective was then to prove that a negative correlation exists, between the NBPs of a project and its followers and forks. 

The previous study has shown us the need to apply different weights to each NBP. 
By diving the NBPs by the size of project we achieved better better results.
However, not all NBPs depend on the project size.
It turned out to be clear, that each NBP should be weight in a different way depending
on the nature of the best practice related and characteristics of the project.
Therefore, we altered the rails best practices gem to make it possible to know how much project files were analyzed 
by each rails best practice checker.
What this means is that if a NBP checker is just trying to find occurrences of errors in the models files of the project
we will weight this result just based in the number and size of models.
This is not the perfect solution yet, but it gives much better results than diving all the best practices by the project size.

In the first study, we analyzed 7 projects, but now we have 40, obviously this time we had to start automating some things.
The work flow to get the information of a project is described in a few steps:
\begin{itemize}
\item \emph{Retrieve GitHub information}, in this step we get the followers and forks(and more info that might be used in further analyses).
\item \emph{Download the project repository}.
\item \emph{Run rails best practices gems}, at this point, we get the non weighted NBPs and files given by each one of the 29 checkers.
\item \emph{Calculate the Weighted Global NBPs}, the evaluation algorithm consists in dividing the value returned by each NBP checker  by the number of files checked and, then sum it.
\end{itemize}

After collecting the GitHub URLs for the 40 project, we used a script to apply the described steps for each project
and stored all the information about the projects in a CSV table.

\input{tables/RBPResults2Raw}
Table~\ref{tab:rbpresults_2_raw} is an excerpt of the obtained table, the full table can be found online: 

\url{http://study.gorgeouscode.com/files/rbp-study.pdf}




\section{Results}\label{subsec:results}
After building the table containing the results for the 40 projects, 
we started trying to find correlation in the different values. 
It was easy to find that our best practice metric was strongly related with the number of forks and watchers of the project.
We discovered that the average correlation index, for the weighted C(x) columns, is -0.2. 
Only three of the weighted C(x) columns do not have negative correlation which is quite a good result. 
Moreover, the positive positive correlation shown in these three columns,
is due the fact that the respective checkers (without negative correlation) are
aimed at finding NBPs that almost non of the projects were committing.
This explains why there is no correlation.
It is obvious that if every project follows a best practice, we can not really use it to distinguish the quality of the project, 
nevertheless it turn it even more clear that it is a something that should be followed.
In the end, when you do not consider those best practices this relation is even more substantial.


The most important results are in the next table:
\input{tables/RBPResults2}

These correlation indexes show that if we just count the nbps there is no relation between them and the number of forks and watchers. Nevertheless, the Weighted NBPs have a quite perceptible negative correlation both with watchers and forks. 

Observing Table~\ref{tab:rbpresults_2}, it is possible to notice that the forks correlation is bigger. 
We believe that happens because forking a project shows intensions of digging into the code and 
it is obviously easier to understand code from other people when it follows best practices.

In addition, after proving that this correlation truly exists we feel now confidante that 
the Weighted NBPs can be used as a metric of quality for Rails projects.
In fact, we have achieved a new metric for classifying the quality of Rails projects in terms of maintainability,
but not only, has it was previously said in this document many of the best practices
proposed by the Rails Best Practices Project members are also related with performance improvements.

However, there is one problem when using the Weighted Global NBPs to certify the quality of a Rails project, 
although it is intuitive to understand that a small number in this metric is a good result
(it means that the project failed few times in terms of best practices), as sad as it is, 
no project of normal size was found with zero occurrences of NPBs.
Consequently, by simply saying that a project has 450 Weighted Global NBPs it is hard for a user or developer
to understand if that is a good or bad result.

Zero is definitely the best possible score, but there is no limit for the worst possible value.
To retrieve some real meaning from this metric we need to compare the project results with other ones.
Therefore, a simple solution for this would be to analyze as many rails projects as possible, to determine and average value.
After that, we would have a reference value, to consider a single project above or under average.

That is exactly what we did next, 
we created a simple web application capable of certifying the quality of Rails projects concerning to the best practices followed. 
By using this database of reports it is possible to dynamically update the average value for the quality of a project 
and use it as a reference value. During this process it is also essential to eliminate any outliers. 
Putting it all together it is just a matter of inverting the Weighted Global NBPs of project and scaling it. 
In the end, we can give a final score to any Rails project in a scale from 1 to 5, being 5 the best possible score.

\begin{figure}[h!]
  \caption{Ruby on Rails Best Practices Report}\label{fig:rbp_s}
  \centering
  \includegraphics[scale=0.75]{Images/rbp_s}
\end{figure}

The mentioned application was developed using Ruby on Rails and it is actually possible to consult an extensive list of analyzed projects online at \url{app.study.gourgeouscode.com}. For each project a page like the show in Figure~\ref{fig:rbp_s} is generated, 
it contains the overall score, a brief possibly explanation for the actual score and a table pointing out the 
NBPs found. If a row shows in red, it means that the score of the project was significantly lowered because 
it does not follow that specific best practice, in other words this should be the first place to improve the code,
which will also increase the quality of the project.

This application was the ultimate result of this studies.












