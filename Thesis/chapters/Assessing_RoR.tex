\thispagestyle{empty}
\chapter{Assessing Ruby on Rails Projects}\label{chap:assissing_ror}


%%--------------------------------------
%% ASSESSING RUBY ON RAILS PROJECTS
\section{Assessing Ruby on Rails Projects} \label{sec:assessing_ror}
After deciding that some \emph{procedure} is a \emph{best practice},
it would be handy to find a way to automatically verify whether that practice is being followed by
the developers of a given project.
With that in mind, an open source ruby gem was created (by the authors of Rails best practices web site) 
with the objective of automatically producing a report that shows where,
in the source code,  a project is failing to obey to consensual practices.
At the moment of writing, this gem can check for 28 kinds of best practices (from the 70 described in that web site).

However, one of the first things that we have noticed when we have applied this gem to OSS projects,
is that the biggest and most renown projects have much more errors than the smaller and unknown projects.
This nonsense has a simple  interpretation.
%The majority of RoR projects found in github are simple projects, in most cases, developed by a single user.
Small projects (like the majority of RoR projects found in github) are simple software packages,  often 
developed by a single user.
These applications are so simple that many times the code is almost entirely created by RoR code generators.
Usually, when code is not written by humans, it has few mistakes concerning those recommendations.

\subsection{First Study}\label{subsec:first_study}
Having taken the above into account, we decided to run the rails best practices gem on similar RoR (Ruby on Rails) projects.
Seven \emph{time tracking} or \emph{project management} open source systems were chosen.
After running the gem and counting
\textsf{not best practices (NBPs)}\footnote{In fact, Rails best practices gem does not find best practices in the source code.
  It does the opposite, it discovers when the code is not written according to a best practice, in other words, 
  it identifies bad practices (similar to the detection of code smells).
  We decided to name those occurrences NBP.
}
occurrences, the following results were obtained:

\begin{table}[H]
\begin{center}{\scriptsize
  \begin{threeparttable}
  \begin{tabular}{|l||c|c|c|c|c|c|c|} \hline
  \multicolumn{8}{|c|}{Rails Best Practices Results} \\ \hline
  \textbf{Best Practice}& \textbf{A}& \textbf{B}& \textbf{C}&  \textbf{D}& \textbf{F}& \textbf{G}& \textbf{H} \\\hline\hline
  \emph{\tnote{a}Add model virtual attribute           }              &   -  &   2  &   7  &   - &   - &   5 &   4  \\ \hline
  \emph{Always add db index                   }              &   -  &   -  &   -  &  43 &   - &   - &  51  \\ \hline
  \emph{Isolate seed data                     }              &   -  &   -  &   -  &   - &   - &  79 &  17  \\ \hline
  \emph{Law of demeter                        }              &  20  &  38  &  45  &   6 &  30 & 164 &  85  \\ \hline
  \emph{Move code into controller             }              &   -  &   -  &   -  &   - &   2 &   - &   4  \\ \hline
  \emph{Move code into model                  }              &   -  &  26  &   -  &   7 &   1 &   3 &  19  \\ \hline
  \emph{Move model logic into model           }              &   -  &   -  &  76  &  11 &  11 &  98 & 100  \\ \hline
  \emph{Move finder to named\_scope           }              &   -  &   4  &   9  &   2 &   4 &  25 &   -  \\ \hline
  \emph{Needless deep nesting                 }              &   -  &   -  &   -  &   1 &   - &   - &   -  \\ \hline
  \emph{Not use default root                  }              &   -  &   1  &   1  &   - &   1 &   1 &   1  \\ \hline
  \emph{Notes  use query attribute            }              &   -  &   2  &   -  &   - &   - &   - &   -  \\ \hline
  \emph{Overuse route customizations          }              &   -  &   -  &   2  &   4 &   - &   2 &   2  \\ \hline
  \emph{Remove trailing whitespace            }              &  68  &  57  & 126  & 110 & 330 & 316 & 100  \\ \hline
  \emph{Use factory method                    }              &   -  &  15  &   9  &   5 &   1 &   8 &  19  \\ \hline
  \emph{Replace instance var with local var   }              &  13  &   -  &  70  & 239 & 142 &  31 & 100  \\ \hline
  \emph{Use before\_filter                    }              &   -  &   7  &   9  &   8 &   8 &  19 &  23  \\ \hline
  \emph{Wrong email content\_type             }              &   -  &   3  &   -  &   - &   - &   - &   -  \\ \hline
  \emph{Use query attribute                   }              &   -  &   -  &  11  &   5 &   8 &  29 &   6  \\ \hline
  \emph{Use say with time in migrations       }              &   -  &   -  &  24  &   - &  10 &  23 &  56  \\ \hline
  \emph{Use scopes access                     }              &   -  &   -  &   -  &   - &   - &   - &  04  \\ \hline
  \emph{User model association                }              &   -  &   -  &  12  &   9 &   - &   1 &  21  \\ \hline
  \emph{Keep finders on their own model       }              &   8  &   4  &   1  &   - &  11 &   - &   -  \\ \hline
  \emph{Total                                 }              & 109  & 156  & 402  & 450 & 559 & 834 & 864  \\ \hline
  \end{tabular}
  \begin{tablenotes}
    \item \emph{A:} Rubytime
    , \emph{B:} Notes
    , \emph{C:} Tracks
    , \emph{D:} Handy Ant
    , \emph{F:} Retrospectiva
    , \emph{G:} Redmine
    , \emph{H:} Clockingit
    \item Figures shown represent the number of times a project do not follow a best practice; is expected that \emph{smaller the number, better the project}.
  \end{tablenotes}
  \end{threeparttable}
}
\end{center}
\caption{Results obtained by running the \emph{best practices analyzer gem} on the 7 Open Source Projects chosen (data produced on April, 2011).}
\end{table}

Rubytime seems to have the best results and Clockingit the worst. 
The fact is that very good user reviews can be found about Rubytime.
However, Tracks obtained an unexpected high score, since it has been very sparsely maintained 
(old code has higher probability of not following the current best practices).
As explained before, those values are not really measuring if a project follows best practices 
but instead measuring when it fails.
This should also be taken into consideration. 

The most evident problem here is that best practices are not being weighted and neither the size of the project considered.
For instance, if the developers have the habit of leaving trailing white spaces, 
the occurrences of this will obviously be related to the size of the project.
On the other hand, it is a best practice to remove the default route generated by rails, 
independently of the project size this is true or false, there is no way to leave the route two times. 
So, if developers do not take into account those two best practices, when the project grows, 
the number of trailing spaces will increase and the results will show more NBPs, 
but the other one will always be only one NBP.  
Because of that we can get twisted results.

To avoid this, the projects were sized.
The size attribute is based on the quantity of models and controllers in the project.
After that, we divided the values previously obtained  by the project size.
By doing that, a new set of results emerge.

\begin{table}[H]
\begin{center}
{\scriptsize
\begin{threeparttable}
\begin{tabular}{|l||c|c|c|c|c|c|c|} \hline
\multicolumn{8}{|c|}{Rails Best Practices Results} \\ \hline
\textbf{Best Practice}& \textbf{A}& \textbf{B}& \textbf{C}&  \textbf{D}& \textbf{F}& \textbf{G}& \textbf{H} \\\hline\hline
\emph{Total                                           }              & 109  & 156  & 402  & 450 & 559 & 834 & 864  \\ \hline
\emph{Total Without Trailing Whitespace               }              &  41  &  99  & 276  & 340 & 229 & 518 & 764  \\ \hline
\emph{Project Size                                    }              &  12  &  11  &  11  &  29 &  26 &  58 &  31  \\ \hline
\emph{Total / Project Size                            }              &   9  &  14  &  37  &  16 &  23 &  15 &  28  \\ \hline
\emph{Total Without Trailing Whitespace / Project Size}              &   3  &   9  &  25  &  12 &   9 &   9 &  25  \\ \hline
\end{tabular}
\begin{tablenotes}
  \item \emph{A:} Rubytime.
  , \emph{B:} Notes
  , \emph{C:} Tracks
  , \emph{D:} Handy Ant
  , \emph{F:} Retrospectiva
  , \emph{G:} Redmine
  , \emph{H:} Clockingit
\end{tablenotes}
\end{threeparttable}
}
\end{center}
\caption{Results obtained by running the \emph{best practices analyzer gem} on the 7 Open Source Projects chosen, after normalization (data produced on April, 2011).}
\end{table}

Those results are much more likely to be helpful in terms of understanding if a project is or is not following 
best practices.
The numbers reflect both the community reviews and our own estimates much more.


\subsection{Second Study}\label{subsec:second_study}
After the first study reported above, we felt that it was time to make a bigger one;
we should repeat the experiment over a larger sample. 
In addition, there was the need to define an objective quality metric to compare the metrics results with.
As a second target for this new phase, it was decide to find an objective quality rate (a reputation ranking) for each project in the sample, 
to be possible to compare with the results computed for the best practices metrics.

For the second study, we selected 40 Ruby on Rails projects hosted in github and
decided to consider the number of 
\textsf{followers}\footnote{Number of users that want to receive notifications about the project.} and
\textsf{forks}\footnote{Number of people that forked the project. This means that either they want to contribute to the project or create a derived project}, 
that each project has on github, 
as a \emph{project reputation} metric. 

The objective was to prove that a negative correlation exists, between the NBPs of a project and its followers and forks. 

The previous study has shown us the need to apply different weights to each NBP. 
By diving the NBPs by the project size, in the first study, seemed like we got better results.
However, not all NBPs depend on the project size. 
Therefore, we altered the rails best practices gem to make it possible to know how much project files were analyzed 
by each rails best practice checker.

Basically, after collecting the GitHub URLs for each project, we followed the next steps:
\begin{itemize}
\item \emph{Retrieve GitHub information}, in this step we get the followers and forks(and more info that might be used in further analyses).
\item \emph{Download the project repository}.
\item \emph{Run rails best practices gems}, at this point, we get the non weighted NBPs and files given by each one of the 29 checkers.
\item \emph{Calculate the Weighted Global NBPs}, the evaluation algorithm consists in dividing the value returned by each NBP checker  by the number of files checked and, then sum it.
\end{itemize}

Next, an excerpt of the obtained table is shown:
\begin{table}[H]
\begin{center}
{\scriptsize
\begin{threeparttable}
\begin{tabular}{|l||c|c|c|c|c|c|c|c|c|c|c|} \hline
\multicolumn{12}{|c|}{Rails Best Practices Results} \\ \hline
Projects & \textbf{Forks}         & \textbf{Watchers} & 
C1       & C1 F.                  & \textbf{W. C1} & 
C2       & C2 F.                  & \textbf{W.       C1} & 
...      & T. NBPs                & \textbf{W.  T. NBPs} \\\hline\hline
\emph{Rails Admin } & 30 & 2478 &  0 & 141 & \textbf{  0 }&  0 &  37 & \textbf{ 0} & ...&  50 & \textbf{ 739}  \\ \hline
\emph{Rubytime    } & 12 &   82 & 24 & 161 & \textbf{149 }&  0 & 134 & \textbf{ 0} & ...& 146 & \textbf{1334}  \\ \hline
\emph{Redmine     } & 30 & 1781 & 49 & 996 & \textbf{ 49 }&  1 & 362 & \textbf{ 2} & ...& 884 & \textbf{1402}  \\ \hline
\emph{BrowserCMS  } & 30 &  784 & 11 & 234 & \textbf{ 47 }&  0 & 216 & \textbf{ 0} & ...& 268 & \textbf{1510}  \\ \hline
\emph{Tracks      } & 17 &   87 & 46 & 842 & \textbf{ 54 }& 15 & 271 & \textbf{55} & ...& 569 & \textbf{2810}  \\ \hline
\emph{...}&...&...&...&...&...&...&...&...&...&...&...\\ \hline
\end{tabular} 

\begin{tablenotes}
  \item{ \emph{C(x): }} The rails best practices gem has 29 checkers(when this study was carried), each one tries to find occurrences of a different nbp in the project. 
  \item{\emph{C(x) Files: }} The number of files in the project, where it tried to find nbps (for instance, some checkers may only be concerned with html files, some other checker nbps my only occur in model files, etc)
  \item{\emph{W. C(x): }} Weighted C(x) = C(X) / C(x)Files * 1000 (A really small number is added to each variable to avoid divisions by zero).
\end{tablenotes}
\end{threeparttable}
}
\end{center}
\caption{Results obtained by running the \emph{best practices analyzer gem} on the 40 Open Source Projects chosen, from GitHub (data produced on April, 2011). The full table can be found at www.bestpracticesstudy.gorgeouscode.com}
\end{table}

\subsection{Results}\label{subsec:results}
After building a table containing the results for the 40 projects, we easily found correlations between columns.
We discovered that the average correlation index, for the weighted C(x) columns, is -0.2. Only three of the weighted C(x) columns do not have negative correlation. This is quite good, considering the fact that there is an explanation for it. 
Those three checkers (without negative correlation) aimed at finding  NBPs that almost non of the projects were committing, 
so there is no correlation.


The most important results are in the next table:
\begin{table}[H]
\begin{center}
{\scriptsize
\begin{threeparttable}
\begin{tabular}{|l||c|c|} \hline
\multicolumn{3}{|c|}{Correlations} \\ \hline
                       & \textbf{Total NBPs}  & \textbf{Total Weighted NBPs}  \\ \hline\hline
\emph{Forks         }  & 0.14                 & -0.53                       \\ \hline
\emph{Watchers      }  & 0.07                 & -0.40                       \\ \hline
\end{tabular}
\end{threeparttable}
}
\end{center}
\caption{ The full table can be found at www.bestpracticesstudy.gorgeouscode.com}
\end{table}

These correlation indexes show that if we just count the nbps there is no relation between them and the number of forks and watchers. Nevertheless, the Weighted NBPs have a quite perceptible negative correlation both with watchers and forks. 

Observing that Table, it is possible to notice that the forks correlation is bigger. 
We believe that if it happens, it is because forking a project shows intensions of digging into the code and, 
of course, it easier to understand others code when it follows good practices.

As future work, we are considering more correlations with other variables that are already available, but we haven't used yet. The most relevant ones: the number of commiters, starting date of the project, last commit data, and total number of commits. We believe that those variables can strongly be related with the forks, watchers and of course, in the end, the quality of the project.

