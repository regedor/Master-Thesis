\thispagestyle{empty}
\chapter{Assessing Ruby on Rails Projects}\label{chap:assissing_ror}

To assess the quality of a software project is not an easy task.
Too many variables should be taken into consideration and most of their values can be considered subjective.

Our thesis is that the quality of an open source project is somehow influenced by the best practices followed 
by its contributors.

Furthermore, since the Rails community has been our object of study, we have decided to conduct a series of studies 
with the objective of finding correlations between best practices defined by the Rails community and
the quality of projects following it.

With the objective of being able to automatically verify whether or not best practices are being followed by
a given Rails project, the open source ruby gem rails\_best\_practices 
was created (by the authors of Rails best practices web site).
We agreed on using it as the starting point for our work.

This chapter reports the different studies carried out, the main difficulties, and the results obtained.



\section{First Study}\label{subsec:first_study}
One of the first things that we have noticed, when we applied the rails\_best\_practices gem to OSS projects,
is that the biggest and most renowned projects have much more errors than the smaller and unknown projects.
This apparent nonsense has a simple  interpretation.
Small projects (like the majority of Rails projects found in GitHub) are simple software packages,
often developed by a single user, and are carried out for simple learning purposes.
These applications are so simple that many times the code is almost entirely created by Rails code generators.
Usually, when code is not written by humans, it has few mistakes concerning those recommendations.

Having taken the above into account, we decided to run the rails best practices gem on similar Rails projects.
Seven \emph{time tracking} or \emph{project management} open source systems were chosen.
After running the gem and counting the
\textsf{not best practices (NBPs)}\footnote{In fact, Rails Best Practices gem does not find best practices in the source code.
  It does the opposite, it discovers when the code is not written according to a best practice, in other words, 
  it identifies bad practices (similar to the detection of code smells).
  We decided to name those occurrences NBP.
}
occurrences, the following results were obtained:

\input{tables/RBPResults1Raw}

Rubytime seems to have the best results and Clockingit the worst. 
The fact is that very good user reviews can be found about Rubytime.
However, Tracks obtained an unexpected high score, since it has been very sparsely maintained 
(old code has higher probability of not following the current best practices).
As explained before, those values are not really measuring if a project follows best practices ,
but instead measuring when it fails.
This should also be taken into consideration. 

The most evident problem here is that best practices are not being weighted, and the size of the project is not being considered.
For instance, if the developers have the habit of leaving trailing white spaces, this will obviously be related to the size of the project.
On the other hand, it is a best practice to remove the default route generated by rails; 
independent of the project size this is either true or false: there is no way to leave the route two times. 
So, if developers do not take into account the differences between these two best practices, when the project grows, 
the number of trailing spaces will increase and the results will show more NBPs. 
However, the other best practice (remove the default route) will always count as only one NBP.  
This can generate twisted results.

To avoid this, the projects were sized.
The size attribute is based on the quantity of models and controllers in the project.
After that, we divided the values previously obtained  by the project size.
Now a new set of results emerge.

\input{tables/RBPResults1}

Those results (in Table~\ref{tab:rbpresults_1}) are much more likely to be helpful in terms of understanding whether or not a project is following 
best practices.
The numbers reflect both the community reviews and our own estimates much more.



\section{Second Study}\label{subsec:second_study}
After the first study reported above, we felt that it was time to conduct a larger one.
We felt that we should repeat the experiment over a larger sample. 
As a second target for this new phase, it was decided to find an objective quality rate (a reputation ranking) 
for each project in the sample.
There was the need to define an objective quality metric to compare the metrics results with.
This way it would be possible to prove that there is a statistical relationship between the quality of the project
and the results of our best practices metric based on NBPs.

For the second study, we selected 40 Ruby on Rails projects hosted in GitHub and
decided to consider the number of 
\textsf{followers}\footnote{Number of users that want to receive notifications about the project.} and
\textsf{forks}\footnote{Number of people that forked the project. This means that either they want to contribute to the project or create a derived project}, 
that each project has on GitHub, 
as a \emph{project reputation} metric. 

The objective was then to prove that a negative correlation exists between the NBPs of a project and its followers and forks. 

The previous study has shown us the need to apply different weights to each NBP. 
By dividing the NBPs by the size of project, we achieved better results.
However, not all NBPs depend on the project size.
It turned out to be clear, that each NBP should be weighted in a different way depending
on the nature of the best practice related to it, and characteristics of the project.
Therefore, we altered the rails best practices gem to make it possible to know how many project files were analyzed 
by each rails best practice checker.
What this means is that if an NBP checker is just trying to find occurrences of errors in the models files of the project
we will weigh this result based on the number and size of models.
This is not the perfect solution yet, but it gives much better results than dividing all the best practices by the project size.

In the first study, we analyzed 7 projects. Now we have 40, and obviously this time we had to start automating some things.
The work flow to get the information of a project is described in a few steps:
\begin{itemize}
\item \emph{Retrieve GitHub information}. In this step we get the followers and forks(and more info that might be used in further analyses).
\item \emph{Download the project repository}.
\item \emph{Run rails best practices gems}. At this point, we get the non weighted NBPs and files given by each one of the 29 checkers.
\item \emph{Calculate the Weighted Global NBPs}. The evaluation algorithm consists of dividing the value returned by each NBP checker  by the number of files checked, and then summing it.
\end{itemize}

After collecting the GitHub URLs for the 40 projects, we used a script to apply the described steps to each project
and stored all the information about the projects in a CSV table.

\input{tables/RBPResults2Raw}
Table~\ref{tab:rbpresults_2_raw} is an excerpt of the obtained table. The full table can be found online: 

\url{http://study.gorgeouscode.com/files/rbp-study.pdf}




\section{Results}\label{subsec:results}
After building the table containing the results for the 40 projects, 
we began searching for a correlation between the different values. 
It was easy to find that our best practice metric was strongly related with the number of forks and watchers of the project.
We discovered that the average correlation index for the weighted C(x) columns is -0.2. 
Only three of the weighted C(x) columns do not have negative correlation, which is a pretty good result. 
Moreover, the positive correlation shown in these three columns
is due to the fact that their respective checkers (without negative correlation) are
aimed at finding NBPs that almost none of the projects were committing.
This explains why there is no correlation.
It is obvious that if every project follows a best practice, we can not really use it to distinguish the quality of the project; 
nevertheless, it makes it even more clear that it is a project that should be followed.
In the end, when you do not consider those best practices, this relation is even more substantial.


The most important results are in the next table:
\input{tables/RBPResults2}

These correlation indexes show that if we just count the NBPs, there is no relation between them and the number of forks and watchers. Nevertheless, the Weighted NBPs have quite the perceptible negative correlation, both with watchers and forks. 

By observing Table~\ref{tab:rbpresults_2}, it is possible to notice that the forks correlation is bigger. 
We believe this happens because forking a project shows intentions of digging into the code and, 
it is obviously easier to understand code from other people when it follows best practices.

After proving that this correlation truly exists, we now feel confident that 
the Weighted NBPs can be used as a metric of quality for Rails projects.
In fact, we have achieved a new metric for classifying the quality of Rails projects in terms of maintainability,
Not only this, as it was previously said in this document, but many of the best practices
proposed by the Rails Best Practices Project members are also related with performance improvements.

However, there is one problem when using the Weighted Global NBPs to certify the quality of a Rails project. 
Although it is intuitive to understand that a small number in this metric is a good result
(meaning that the project failed few times in terms of best practices), sadly 
no project of normal size was found with zero occurrences of NPBs.
Consequently, simply saying that a project has 450 Weighted Global NBPs makes it harder for a user or developer
to understand if this is a good or bad result.

Zero is definitely the best possible score, but there is no limit for the worst possible value.
To retrieve some real meaning from this metric, we need to compare the project results with other ones.
Therefore, a simple solution to this would be to analyze as many rails projects as possible and to determine and average value.
After that, we would have a reference value from which to consider a single project above or under the average.

That is exactly what we did next: 
we created a simple web application capable of certifying the quality of Rails projects based on the best practices followed. 
By using this database of reports, it is possible to dynamically update the average value for the quality of a project 
and use it as a reference value. It is also essential to eliminate any outliers during this process. 
Putting it all together is just a matter of inverting the Weighted Global NBPs of projects and scaling them. 
In the end, we can give a final score to any Rails project in a scale from 1 to 5, 5 being the best possible score.

\begin{figure}[h!]
  \caption{Ruby on Rails Best Practices Report}\label{fig:rbp_s}
  \centering
  \includegraphics[scale=0.75]{Images/rbp_s}
\end{figure}

The mentioned application was developed using Ruby on Rails, and it is actually possible to consult an extensive list of analyzed projects online at \url{app.study.gorgeouscode.com}. For each project, a page like the table in Figure~\ref{fig:rbp_s} is generated, 
which contains the overall score, a brief possible explanation for the actual score, and a table pointing out the 
NBPs found. If a row is shown in red, it indicates that the score of the project was significantly lowered because 
it does not follow that specific best practice. In other words, this should be the first place to improve the code,
which will also increase the quality of the project.

This application was the ultimate result of these studies.












